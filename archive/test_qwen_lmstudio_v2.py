# File: test_qwen_lmstudio_v2.py
# Purpose: A self-contained script to test a single scenario with our final,
#          production-quality prompt against a model
#          served by LM Studio.

import os
import sys
import base64
from io import BytesIO
from PIL import Image
from openai import OpenAI

# --- Add project root to path for our src imports ---
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

from src.utils.config_loader import load_config

# --- Helper function to encode images for the API payload ---
def pil_image_to_data_uri(image: Image.Image) -> str:
    """Converts a PIL Image object to a base64-encoded data URI."""
    buffered = BytesIO()
    image.save(buffered, format="JPEG")
    img_str = base64.b64encode(buffered.getvalue()).decode('utf-8')
    return f"data:image/jpeg;base64,{img_str}"

def main():
    """
    Main function to run the test.
    """
    print("--- Testing Qwen with Final V7 Prompt via LM Studio Server ---")

    # --- 1. Client and Config Setup ---
    try:
        config = load_config()
        # Ensure you are using the correct IP if your LM Studio server is on a different machine
        # You can find this IP in the LM Studio server tab.
        base_url = "http://192.168.1.67:1234/v1" 
        client = OpenAI(base_url=base_url, api_key="not-needed")
    except Exception as e:
        print(f"❌ Failed to load configuration. Error: {e}")
        return

    # --- 2. Load Visual Assets (Legend + Specific Scenario) ---
    try:
        scenario_id_to_test = "599e00c98611394e" # The challenging Red Light Violation scenario
        preprocessed_dir = "outputs/preprocessed_scenarios"
        legend_assets_dir = "outputs/legend_assets"
        
        # Load the single visual legend image
        legend_image_path = os.path.join(legend_assets_dir, "visual_legend.png")
        legend_image = Image.open(legend_image_path).convert("RGB")
        
        # Load scenario GIF and select keyframes
        gif_path = os.path.join(preprocessed_dir, scenario_id_to_test, "scenario.gif")
        gif_image = Image.open(gif_path)
        key_frame_indices = [int(gif_image.n_frames * p) for p in [0.1, 0.2, 0.4, 0.6, 0.8, 0.9]]
        key_frames = [gif_image.seek(i) or gif_image.convert("RGB").copy() for i in key_frame_indices]
        
        print(f"✅ Loaded visual assets for scenario {scenario_id_to_test}.")
    except FileNotFoundError as e:
        print(f"❌ ERROR: Could not find a required asset file. {e}")
        print("   Please ensure you have run 'create_legend_image.py' and 'preprocess_scenarios.py' first.")
        return

    # --- 3. Define the Final V7 Prompt Components ---
    
    # Component A: The System Prompt (Simplified for visual focus)
    system_prompt = """You are a meticulous and objective Autonomous Vehicle Safety Analyst. Your sole purpose is to audit driving scenarios based on the visual evidence provided.

--- CRITICAL CONTEXT: HOW TO INTERPRET THE VISUALIZATION ---
The images you are about to see are **not photographs**. They are a simplified, top-down schematic visualization generated by a custom Python simulation tool, similar to professional traffic simulators like SUMO or VISSIM. The data is from the **Waymo Open Motion Dataset**.

You MUST interpret the visuals according to these rules:

1.  **Ego-Centric Perception:** The visualization represents the world as perceived by the Magenta AV. You are seeing what the AV's sensors recorded.
2.  **Occlusions are Real:** Objects can and will disappear if they are occluded (hidden) by other vehicles or map elements from the AV's line of sight. A disappearing object is not a glitch; it is a representation of a real-world occlusion.
3.  **Partial Views are Intentional:** You may only see the traffic lights that are relevant to the AV's lane. The simulation does not render traffic lights for cross-traffic if they are not in the AV's direct field of view. A "missing" traffic light is an intentional feature of the data.
4.  **Colors are Schematic:** The colors are for identification only and do not represent real-world colors. You must use the provided visual legend to understand their meaning.

Your Core Directives:
1.  **Trust the Visuals:** Base your analysis on the events as they are depicted, following the rules above.
2.  **Acknowledge Perception Limits:** When identifying risks, explicitly consider the AV's limited perception (e.g., "The risk is high because the truck is occluding a potential pedestrian").
3.  **Think in Causal Chains:** Connect actions to consequences to identify the root causes of risk.
"""
    # Component B: The User Task Prompt (Chain-of-Thought, asking for a counterfactual)
    user_task_prompt = """
First, perform a detailed, step-by-step analysis of the scene before providing your final answer. 
The images are from a visualization of a traffic system, similar to a traffic simulation software (like SUMO, VISSIM, or a similar tool).
The data are extracted from Open Waymo Motion Dataset, using a custom code in python to generate the traffic scenarios.

Follow this exact process:

**Internal Monologue (Chain-of-Thought):**
1.  **Scene Description:** Describe the static environment in detail.
2.  **Dynamic Analysis (Frame-by-Frame):** Describe the actions of all agents.
3.  **Synthesize Key Events:** Summarize the most critical event.

**Final Answer:**
Now, based on your detailed analysis above, provide your final answer in the following three-step format:

**Step 1: Factual Event Chronology.**
Concisely summarize the sequence of events.

**Step 2: Causal Risk Identification.**
Identify the primary causal risk or traffic law violation. Explain the causal chain.

**Step 3: Optimal Action at the Critical Moment.**
The critical moment is the situation depicted in **Frame 1 and Frame 2**. What is the single, safest action the Magenta AV **should have taken** instead of its actual maneuver? Justify your recommendation.
"""

# --- 4. Construct the Final OpenAI-compatible Messages List ---
    user_content = []
    
    # Part 1: Visual Legend and its introduction
    user_content.append({"type": "text", "text": "Use this visual legend to identify all objects in the upcoming scenario:"})
    user_content.append({
        "type": "image_url",
        "image_url": {"url": pil_image_to_data_uri(legend_image)}
    })
    
    # Part 2: The main task description
    user_content.append({"type": "text", "text": f"\n\n{user_task_prompt}\n\n--- Scenario Keyframes ---"})
    
    # Part 3: The scenario keyframes, interleaved with their text labels
    for i, frame in enumerate(key_frames):
        frame_idx = key_frame_indices[i]
        user_content.append({"type": "text", "text": f"\n**Frame {i+1} (Original Timestep: {frame_idx})**"})
        user_content.append({
            "type": "image_url",
            "image_url": {"url": pil_image_to_data_uri(frame)}
        })

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_content}
    ]

    # --- 5. Run Inference via API Call to LM Studio ---
    print("\n--- Sending final V7 prompt to LM Studio server... ---")
    try:
        response = client.chat.completions.create(
            model="local-model", # This is a placeholder for LM Studio
            messages=messages,
            max_tokens=2048,     # Increased for the verbose CoT output
            temperature=0.1      # Low temperature for deterministic, factual output
        )
        output_text = response.choices[0].message.content
        print("\n--- Qwen Model (Final Prompt) LM Studio Output ---")
        print(output_text)
    except Exception as e:
        print(f"\n❌ ERROR: Could not connect to the LM Studio server.")
        print(f"   Please ensure the server is running on {base_url} and the model is fully loaded.")
        print(f"   Error details: {e}")

    print("\n--- END OF EXPERIMENT ---")

if __name__ == "__main__":
    # Ensure the openai library is installed: pip install openai
    main()