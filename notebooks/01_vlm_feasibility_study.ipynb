{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 01: VLM Feasibility Study for Causal Reasoning\n",
    "\n",
    "**Author:** Antonio Guillen-Perez\n",
    "**Date:** November 12, 2025\n",
    "\n",
    "## 1. Objective\n",
    "\n",
    "The primary goal of this study is to validate the core hypothesis of the \"Causal Scrutinizer\" project: **Can a powerful, open-source Vision-Language Model (VLM), running on local hardware (a single RTX 3090), perform high-quality causal reasoning on complex driving scenarios?**\n",
    "\n",
    "This notebook documents the final, successful experiment in our feasibility study, which proves that a VLM can indeed understand and reason about a *sequence of events* when presented with a clean, schematic visual input. This result provides the green light to proceed with the main engineering phase of the project: building a full-scale Scenario Renderer for the Waymo Open Motion Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup: Loading the GGUF Model with `llama-cpp-python`\n",
    "\n",
    "After initial experiments with `transformers` and `bitsandbytes` revealed numerical instability with on-the-fly quantization for large models like Gemma 3, we pivoted to a more robust and stable inference toolchain.\n",
    "\n",
    "Our final, stable stack consists of:\n",
    "- **Model:** `google/gemma-3-12b-it-qat-q4_0-gguf` - The official, quantization-aware trained 4-bit version of Gemma 3 12B.\n",
    "- **Vision Projector:** `mmproj-model-f16-12B.gguf` - The corresponding multimodal projector required to connect the vision encoder to the language model.\n",
    "- **Inference Backend:** `llama-cpp-python` with full CUDA support, which is highly optimized for running GGUF models on local GPUs.\n",
    "\n",
    "The following code cell initializes this entire pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from llama_cpp.llama_chat_format import Llava15ChatHandler\n",
    "from PIL import Image\n",
    "from IPython.display import display, Image as IPImage\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "# --- 1. SETUP THE MODEL --- \n",
    "# Define paths relative to the project root directory\n",
    "MODEL_DIR = \"../models/\"\n",
    "LLM_FILENAME = \"gemma-3-12b-it-q4_0.gguf\"\n",
    "PROJECTOR_FILENAME = \"mmproj-model-f16-12B.gguf\"\n",
    "\n",
    "llm_path = os.path.join(MODEL_DIR, LLM_FILENAME)\n",
    "projector_path = os.path.join(MODEL_DIR, PROJECTOR_FILENAME)\n",
    "\n",
    "# Check if model files exist\n",
    "if not all(os.path.exists(p) for p in [llm_path, projector_path]):\n",
    "    print(f\"ERROR: Model or projector file not found in {MODEL_DIR}.\")\n",
    "    print(\"Please download the GGUF files from the Hugging Face Hub.\")\n",
    "else:\n",
    "    print(\"--- Initializing Chat Handler with Vision Projector... ---\")\n",
    "    # Note: Llama.from_pretrained will download from HF if repo_id is used.\n",
    "    # We are loading from local paths directly.\n",
    "    chat_handler = Llava15ChatHandler(clip_model_path=projector_path)\n",
    "    \n",
    "    print(\"--- Chat Handler loaded. Initializing GGUF model... ---\")\n",
    "    llm = Llama(\n",
    "        model_path=llm_path,\n",
    "        chat_handler=chat_handler,\n",
    "        n_gpu_layers=-1,  # Offload all layers to GPU\n",
    "        n_ctx=8096,       # Set context window size\n",
    "        verbose=False\n",
    "    )\n",
    "    print(\"--- Multimodal model loaded successfully. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Core Experiment: Causal Analysis of a Dynamic Scenario\n",
    "\n",
    "This experiment tests the VLM's ability to analyze a sequence of events. We will provide it with several keyframes from a simple, programmatically generated GIF of a collision scenario. \n",
    "\n",
    "The keyframes are selected to tell a story:\n",
    "1.  **Frame 1:** The initial approach of both vehicles.\n",
    "2.  **Frame 2:** The moment of imminent risk, just before impact.\n",
    "3.  **Frame 3:** The moment of impact.\n",
    "4.  **Frame 4:** The final state after the collision.\n",
    "\n",
    "The prompt is designed to explicitly ask the model to move beyond simple description and perform a causal analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Display the GIF for context ---\n",
    "GIF_PATH = \"archive/assets/collision_scenario_v2.gif\"\n",
    "print(f\"Displaying the input scenario from: {GIF_PATH}\")\n",
    "display(IPImage(filename=GIF_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. PREPARE MULTI-IMAGE INPUTS ---\n",
    "\n",
    "# Helper function to convert PIL Image to a Data URI\n",
    "def pil_image_to_data_uri(image: Image.Image) -> str:\n",
    "    \"\"\"Converts a PIL Image object to a base64-encoded data URI.\"\"\"\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"JPEG\")\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "    return f\"data:image/jpeg;base64,{img_str}\"\n",
    "\n",
    "# Load the GIF and select keyframes\n",
    "print(f\"--- Loading GIF and selecting keyframes from '{GIF_PATH}'... ---\")\n",
    "gif_image = Image.open(GIF_PATH)\n",
    "\n",
    "# Our \"storytelling\" keyframe selection strategy\n",
    "key_frame_indices = [5, 10, 15, gif_image.n_frames - 1] # Early approach -> Imminent risk -> Impact -> Final state\n",
    "print(f\"Selected keyframe indices: {key_frame_indices}\")\n",
    "\n",
    "key_frames = []\n",
    "for i in key_frame_indices:\n",
    "    gif_image.seek(i)\n",
    "    key_frames.append(gif_image.convert(\"RGB\").copy())\n",
    "\n",
    "# Construct the structured prompt\n",
    "system_prompt = \"You are a world-class expert in autonomous vehicle safety and causal reasoning. Your task is to analyze a sequence of visual frames and provide a clear, concise, and logical explanation of the events.\"\n",
    "user_prompt_text = (\n",
    "    \"The following images are sequential keyframes from a simplified, top-down traffic scenario. \"\n",
    "    \"The red and blue shapes are vehicles.\\n\\n\"\n",
    "    \"1. **Describe the sequence of events** from the first frame to the last.\\n\"\n",
    "    \"2. **Identify the primary causal event** that defines this scenario and explain your reasoning.\\n\"\n",
    "    \"3. Based on the vehicle dynamics, what was the most critical safety failure?\"\n",
    ")\n",
    "\n",
    "# Build the content list for the user message\n",
    "user_content = []\n",
    "for frame in key_frames:\n",
    "    user_content.append({\"type\": \"image_url\", \"image_url\": {\"url\": pil_image_to_data_uri(frame)}})\n",
    "user_content.append({\"type\": \"text\", \"text\": user_prompt_text})\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_content}\n",
    "]\n",
    "\n",
    "# --- 3. RUN INFERENCE ---\n",
    "print(\"\\n--- Generating response for multi-image input... ---\")\n",
    "response = llm.create_chat_completion(\n",
    "    messages=messages,\n",
    "    max_tokens=700,      # Allow for a detailed response\n",
    "    temperature=0.1      # Keep the output factual and deterministic\n",
    ")\n",
    "\n",
    "assistant_response = response['choices'][0]['message']['content']\n",
    "\n",
    "print(\"\\n--- MODEL OUTPUT (Gemma 3 12B - Multi-Frame Causal Analysis) ---\")\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis and Conclusion\n",
    "\n",
    "The model's output is a definitive success and validates our core hypothesis.\n",
    "\n",
    "**Key Successes:**\n",
    "1.  **Temporal Understanding:** The model correctly narrated the sequence of events, identifying the approach, entry into the intersection, and the final collision.\n",
    "2.  **Causal Inference:** It successfully identified the **\"failure of one or both vehicles to yield the right-of-way\"** as the *primary causal event*, correctly distinguishing the cause from the effect (the collision itself).\n",
    "3.  **Application of Latent Knowledge:** The model applied external knowledge about traffic rules (\"right-of-way\") to the abstract scene, demonstrating a deep, generalizable understanding of the world.\n",
    "4.  **Expert Framework:** It correctly broke down the critical safety failure into the standard AV stack components (Perception, Prediction, Decision-Making), adopting the persona of a true safety expert.\n",
    "\n",
    "**Conclusion:** The feasibility study is complete and successful. We have proven that a locally-run Gemma 3 12B model can perform sophisticated, multi-frame causal reasoning on schematic visual inputs. This provides a strong, evidence-based foundation to proceed with the primary engineering task of building the full `ScenarioRenderer` for the Waymo dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}